{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузка необходимых библиотек и модулей\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Embedding, LSTM, Bidirectional\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "from sklearn.preprocessing import normalize\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка данных из Домотехники"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Датасет из Домотехники\n",
    "DT_csv = 'csv/dt_reviews.csv'\n",
    "DT_df = pd.read_csv(DT_csv, sep=';').dropna()\n",
    "\n",
    "# Метод для сжатия градации в данных из Домотехники (1 и 2 становятся 1 (плохо), 4 и 5 становятся 0 (хорошо))\n",
    "def change_rating_value(df, before, after):\n",
    "    df['rating'][df['rating'] == before] = after\n",
    "    return df\n",
    "\n",
    "# Метод подготовка текста для Токинайзера\n",
    "def clear_text(text):\n",
    "    text = text.replace(\"\\\\\", \" \").replace(u\"╚\", \" \").replace(u\"╩\", \" \")\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\-\\s\\r\\n\\s{1,}|\\-\\s\\r\\n|\\r\\n', ' ', text)\n",
    "    text = re.sub('[.,:;_%©?*,!@#$%^&()\\d]|[+=]|[[]|[]]|[/]|\"|\\s{2,}|-', ' ', text)\n",
    "    text = re.sub('[^а-яА-Я ]', ' ', text)\n",
    "    text = re.sub(' +', ' ', text)\n",
    "\n",
    "    return text\n",
    "\n",
    "change_rating_value(DT_df, 4, 5)\n",
    "change_rating_value(DT_df, 5, 0)\n",
    "change_rating_value(DT_df, 2, 1)\n",
    "\n",
    "# Удаление записей с рейтингом 3 (Теперь между плохими и хорошими отзывами есть \"пропасть\")\n",
    "DT_df = DT_df[~(DT_df.rating==3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Удаляем ненужные столбцы\n",
    "DT_df = DT_df.drop(['#', 'limits', 'comment'],axis=1)\n",
    "\n",
    "# uint- целые числа без знака позволяют хранить столбцы с положительными числами более эффективно\n",
    "DT_df['rating'] = DT_df['rating'].astype('uint8')\n",
    "\n",
    "# Переименовываем столбцы\n",
    "DT_df = DT_df.rename(columns={'rating': 'toxic','accomps': 'comment'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 38940\n",
    "DT_df['comment'] = DT_df['comment'].apply(clear_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обработка данных из Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Датасет из Kaggle\n",
    "kaggle_csv = 'csv/labeled.csv'\n",
    "kaggle_df = pd.read_csv(kaggle_csv)\n",
    "\n",
    "# Подготавливаем текст\n",
    "kaggle_df['comment'] = kaggle_df['comment'].apply(clear_text)\n",
    "# Токсичность может быть 0 или 1, поэтому меняем тип rating с float на uint8\n",
    "kaggle_df['toxic'] = kaggle_df['toxic'].astype('uint8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание единого DataFrame'а из данных с двух источников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "frames = [kaggle_df, DT_df]\n",
    "main_df = pd.concat(frames)\n",
    "\n",
    "# Перемешиваем данные\n",
    "main_df = main_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Подготовка Tokenizer'а "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Токенайзер создает словарь, в котором будет хранить 10000 наиболее часто встречающихся слов из DataFrame'а\n",
    "tokenizer = Tokenizer(num_words=10000, filters='!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', \n",
    "                      lower=True, \n",
    "                      split=' ', \n",
    "                      char_level=False)\n",
    "tokenizer.fit_on_texts(main_df['comment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Преобразует слова в числа (модели работают с токенами)\n",
    "matrix = tokenizer.texts_to_matrix(main_df['comment'], mode='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Нормализованные данные - это данные от 0 до 1 (было от 0 до 15000), модели проще обучаться на числах в небольшом диапазоне \n",
    "normalize_matrix = normalize(matrix)\n",
    "labels = np.array(main_df['toxic'])\n",
    "\n",
    "# Разделение данных на обучающие и тестовые ((36032, 10000), (9009, 10000), (36032,), (9009,))\n",
    "data_train, data_test, labels_train, labels_test = train_test_split(normalize_matrix, labels, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model():\n",
    "#     Модель с точностью в 85%\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(16, activation='relu'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    model.compile(optimizer=RMSprop(lr=0.0001), \n",
    "                  loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "\n",
    "model_history = model.fit(data_train, \n",
    "                    labels_train, \n",
    "                    epochs=70, \n",
    "                    batch_size=500,\n",
    "                    validation_data=(data_test, labels_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = model_history.history\n",
    "fig = plt.figure(figsize=(20, 10))\n",
    "\n",
    "ax1 = fig.add_subplot(221)\n",
    "ax2 = fig.add_subplot(223)\n",
    "\n",
    "x = range(70)\n",
    "\n",
    "ax1.plot(x, history['acc'], 'b-', label='Accuracy')\n",
    "ax1.plot(x, history['val_acc'], 'r-', label='Validation accuracy')\n",
    "ax1.legend(loc='lower right')\n",
    "\n",
    "ax2.plot(x, history['loss'], 'b-', label='Losses')\n",
    "ax2.plot(x, history['val_loss'], 'r-', label='Validation losses')\n",
    "ax2.legend(loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Сохранение и загрузка моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# Модуль для работы с моделями (сохранить / загрузить), чтобы каждый раз не создавать её заново\n",
    "from keras import models\n",
    "\n",
    "def save_model(model, model_name):\n",
    "    model.save(model_name+'.h5')\n",
    "    \n",
    "def load_model(model_name):\n",
    "    return models.load_model(model_name+'.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Загрузка модели и токенайзера"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модуль для работы с Токенайзером (сохраненить / загрузить), чтобы каждый раз не создавать его заново\n",
    "import pickle\n",
    "\n",
    "def write_to_pickle(data, file_name):\n",
    "    with open(file_name + '.pickle', 'wb') as file:\n",
    "        pickle.dump(data, file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def read_from_pickle(file_name):\n",
    "    with open(file_name + '.pickle', 'rb') as file:\n",
    "        return pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Проверка модели "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(data_test, labels_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
